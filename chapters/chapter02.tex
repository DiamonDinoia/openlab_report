The HLT shares the same code used for offline data processing, thus the processing pipeline is more or less the same. The fundamental difference is that the output is used to perform event classification instead of data analysis. 
\begin{figure}[ht]
  \centering
  \caption{HLT processing pipeline}
  \label{img:hlt}
  \includegraphics[width=.75\textwidth]{img/hlt}
\end{figure}
The table \ref{table:timeshare} shows how much time is spent in every single step of the reconstruction process. Most of it is spent into tracking but after it, the second more time consuming step is HCAL+ECAL local reconstruction that takes $113 ms$ corresponding to $24\%$ of the total time.
\begin{figure}[ht]
  % \centering
  \caption{Data processing time share}
  \label{img:hlt}
  \includegraphics[width=\textwidth]{img/timeshare}
\end{figure}
Given the time needed to perform this reconstruction even achieving a speedup of two would reduce the total processing time of more than $10\%$. This is my focus in this project try to reduce it as much as possible.
\begin{table}[ht]
  \caption{Time spent into the various HLT reconstruction steps}
  \label{table:timeshare}
  \begin{tabular}{lll}
    \hline
    Step                      & Real-Time      & Percentage \\ \hline
    ECAL local reconstruction & 38.9 ms        & 8.25\%     \\
    HCAL local reconstruction & 73.9 ms        & 15.67\%    \\
    Jets/MET                  & 14 ms          & 2.97\%     \\
    E/Gamma                   & 20.4 ms        & 4.33\%     \\
    Muons                     & 34.2 ms        & 7.25\%     \\
    Pixel tracking            & 65.7 ms        & 13.93\%    \\
    Full tracking             & 114.2 ms       & 24.22\%    \\
    Vertex reconstruction     & 2.3 ms         & 0.49\%     \\
    Particle Flow and Taus    & 36.8 ms        & 7.8\%      \\
    HLT                       & 14.7 ms        & 3.12\%     \\
    Overhead                  & 56.4 ms        & 11.96\%    \\
    Total                     & 471.5 ms       & 100\%      \\ \hline
  \end{tabular}
\end{table}
\section{Problem statement}
As the name says the goal of this step is: \\
\begin{claim}
  For each channel \{given $n$ charge readouts $\rightarrow$ reconstruct the energy\}. \\
  \quad Where in this case $n$ is fixed to 10. 
\end{claim}\\
Translated into mathematical terms this means:
\begin{equation}\label{eq:chisq}
  \begin{split}
    & \min(\chi^2)=\arg min_x(\norm{Px - b}) \\
    & \forall x: x \geq 0\\
    \text{where:}&  \\
    & x = \text{energy vector} \\
    & P:CHARGE \rightarrow ENERGY =\text{feature matrix} \\
    & b = \text{charge vector}
  \end{split}
\end{equation}
Which is a min$\chi^2$ problem with additional positivity constraints. This constraint is present because physically speaking negative energy does not make sense. \\
As shown in \cite{amplituamplitude_reconde_recon} the statement above is incomplete. A perfect mapping from charges to energy does not exist because signal from the shower does not dissipate within one time slice ($25ns$). Adding the correlation term this problem becomes:
\begin{equation}\label{eq:constrained_chi_sq}
  \begin{split}
      &\arg min_x(\norm{(Px - b)^T\Sigma(x)^{-1}(Px - b)}) \\
      & \forall x: x \geq 0\\
  \end{split}
\end{equation}
It is worth pointing out that $\Sigma$ depends on x, meaning also $\Sigma$ is unknown. \\
To compute $\Sigma$ an iterative procedure is used:
\begin{enumerate}
  \item Compute $\Sigma$.
  \item Minimize $\chi^2$.
  \item If not convergence goto 1.
\end{enumerate}
More precisely $\Sigma$ is the covariance matrix representing the noise correlation between time samples i and j, obtained from data where no signal is present, and the single sample noise.\\
To solve the problem stated in \ref{eq:chisq} several algorithms exists, for example \textbf{lsqnonneg} illustrated in \cite{nnls} and the \textbf{ffnls} illustrated in \cite{fnnls}. The one implemented is fnnls since as measured in \cite{Chen09nonnegativityconstraints} it is faster.\\
The problem presented in \ref{eq:constrained_chi_sq} is not a $\chi^2$ problem but to solve it with nnls needs to be reduced into the canonical form. The redution exploits the Cholesky decomposition and is illustrated in \ref{eq:reduction}.
\begin{equation}\label{eq:reduction}
  \begin{split}
  & (Px-b)^T\Sigma(x)^-1(Px-b)\\
  \equiv &\quad \Sigma=LL^T,\space (AB)^{-1} = B^{-1} A^{-1}\\
  & (Px-b)^T L^{-T} L^{-1} (Px-b)\\
  \equiv &\quad (AB)^T = B^T A^T \\
  & (L^{-1}Px-L^{-1}b)^T  L^{-1} (Px-b)\\
  \equiv & \\
  & (L^{-1}Px-L^{-1}b)^T (L^{-1}Px-L^{-1}b)\\
  \equiv &\quad L^{-1}P=P', \space L^{-1}b=b' \\
  & (P'x-b')^T(P'x-b')\\
  \equiv & \\
  & min(\chi^2)
  \end{split}
\end{equation}

\section{Fast non negative least square algorithm (FNNLS)}
The nnls is an \textit{active set iterative algorithm}. It uses two sets: 
\begin{itemize}   
\item \textbf{Passive set (P)}: the constraint is "passive", meaning that is not satisfied.   
\item \textbf{Active set (R)}:  the constraint is "active", meaning that is satisfied.   
\end{itemize}   
The algorithm presented in pseudo-code \ref{alg:nnls}, starts with a feasible solution (line 2), then checks for the positivity constraint. If there are some negative components exploiting a gradient it finds a non negative one that minimize the error. More details can be founs in \cite{nnls}

\begin{algorithm}[h]
  \begin{flushleft}
  \caption{NNLS}
  \label{alg:nnls}
  \textbf{Input:} \\
  \hspace*{\algorithmicindent} \text{\textbf{A} real valued matrix of dimension $m \times n$}\\
  \hspace*{\algorithmicindent} \text{\textbf{b} real valued vector of dimension $m$}\\
  \hspace*{\algorithmicindent} \text{{$\boldsymbol\epsilon$} the maximum accepted error} \\
  \hspace*{\algorithmicindent} \text{\textbf{K} the maximum number of iterations} \\
  \textbf{Output:} \\
  \hspace*{\algorithmicindent} \text{\textbf{$x$} the solution vector}
  \end{flushleft}
  \begin{algorithmic}[1]
    \Function{nnls}{$\protect{A}$, $\protect{b}$, $\protect{\epsilon}$, $\protect{K}$}
      \State $x \gets 0$
      \State $P=\emptyset$
      \State $R=\{ 1, 2, ..., m \}$
      \State $w=A^T(b - Ax)$ \Comment{compute the gradient}
      \While{$R \neq \emptyset \land max(w) < \epsilon \land k < K$}
        \State $j \gets max(w^P)$ \Comment{$w^P \gets \{w_j : j \in P\}$}
        \State Add $j$ to $P$
        \State Remove $j$ from $R$
        \State $A^P \gets  \{a_{ij} \in A : i \in P \land j \in P\}$
        \State $s \gets ((A^P)^T A^P)^{-1}A^P b^P$ \Comment{s is a vector of the same dimension of P }
        \While{$min(s) \leq 0 $}
          \State $\alpha=min_i\{\frac{x_i}{x_i-s_i} : i \in P \land s_i \leq 0 \}$
          \State $ \forall i \in P : x_i \gets x_i + \alpha (s_i - x_i)$
          \State move to $R$ all $i \in P : x_i \gets 0$
          \State $s \gets ((A^P)^T A^P)^{-1}A^P b^P$ \Comment{recompute s for the next iteration}
          \EndWhile
        \State $\forall i \in P : x_i \gets s_i$
        \State $w \gets A^T(b - Ax)$
        \State $k \gets k+1$
      \EndWhile
    \EndFunction
  \end{algorithmic}
\end{algorithm}
This algorithm is slow because at each iteration it requires to calculate the pseudo-inverse (line 11). 
FNNLS, showed in algorithm \ref{alg:fnnls}, is faster because it reduces the computational burden of this operation. The idea is simple: instead of projecting the matrix $A$ over $P$ and then performing the transposition and multiplication, it saves $A^TA$ and performs the projection over $P$. 
Another operation avoided is the multiplication between $A$ and $b$, also in this case the multiplication is performed in the preprocessing phase. At runtime, only the projection over $P$ is performed. \\
This improvements reduces the computational making the it to run faster. 
\begin{algorithm}[h]
  \caption{FNNLS}
  \label{alg:fnnls}
  \begin{algorithmic}[1]
    \State $w \gets (A^T b) (A^T A)x $
    \State $s \gets ((A^TA)^P)^{-1} (A^Tb)^P$
  \end{algorithmic}
\end{algorithm}