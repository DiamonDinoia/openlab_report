In all tests we did not manage to generate enough threads to fill all the warps. In this case the speedup showed in table \ref{tab:speedup} is of a factor of $\bm{2.67}$ but by increasing the size of the dataset better results should be expected. Also, the CPU implementation optimized with all the numerical, algorithmic, and architecture dependent tweaks reaches a speedup of $\bm{1.38}$. This without changing the intrinsic algorithmic complexity is a very good result.\\
\begin{table}[H]
  \caption{Speedup achieved after optimizing the matrix multiplication.}
  \label{tab:speedup}
\begin{tabular}{lrrrrrrr}
\toprule
channels &  1024  &  2048  &  4096  &  8192  &  16384 &  32768 &  65536 \\
\midrule
legacy\_multifit\_cpu &   1.00 &   1.00 &   1.00 &   1.00 &   1.00 &   1.00 &   1.00 \\
legacy\_multifit\_gpu &   0.62 &   1.22 &   1.77 &   2.09 &   1.89 &   2.10 &   2.23 \\
multifit\_cpu        &   1.25 &   1.38 &   1.35 &   1.35 &   1.33 &   1.34 &   1.34 \\
multifit\_gpu        &   0.96 &   1.57 &   2.09 &   2.56 &   2.31 &   2.54 &   2.67 \\
multifit\_gpu\_swap   &   0.42 &   0.73 &   1.10 &   1.15 &   1.11 &   1.21 &   1.23 \\
multifit\_cpu\_swap   &   0.45 &   0.48 &   0.51 &   0.50 &   0.50 &   0.50 &   0.50 \\
\bottomrule
\end{tabular}
\end{table}
There are few things left to try:
\begin{itemize}
  \item Dynamic parallelism on GPU to better exploit the available resources on heavy numerical computations and to free up the unneeded ones.
  \item Update the Cholesky instead of recomputing it.

\end{itemize}